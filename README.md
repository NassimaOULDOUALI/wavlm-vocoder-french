# WavLM Vocoder for French ğŸ™ï¸ğŸ‡«ğŸ‡·

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)

> **Vocodage WavLM vers audio en franÃ§ais : Ablation des couches et supervision adversariale**  
> Neural vocoder for reconstructing high-quality French speech from WavLM representations

ğŸ“„ **Paper**: [Vocodage WavLM vers audio en franÃ§ais](docs/paper/JEP_2026_WavLM_Vocodeur.pdf)  
ğŸ¯ **Goal**: Stage 1 foundation for continuous voice conversion in WavLM latent space

---

## ğŸ¯ Overview

This repository implements a neural vocoder that reconstructs audio from frozen **WavLM-Base+** representations, specifically trained and evaluated on French speech corpora. 

### Key Features

- âœ… **WavLM-Base+ Integration**: Frozen 12-layer transformer encoder (768-dim)
- âœ… **HiFi-GAN Generator**: Progressive upsampling (Ã—320) with multi-receptive field residual blocks
- âœ… **Layer Ablation Study**: Systematic evaluation of N last layers (N=1...12)
- âœ… **Learned Layer Fusion**: Weighted combination vs. simple averaging
- âœ… **Adversarial Training**: MPD/MSD discriminators + Feature Matching
- âœ… **French Corpora**: SIWIS (10.9h) + M-AILABS (160.7h) + Common Voice (66.7h) = 238.3h
- âœ… **Comprehensive Metrics**: MCD, PESQ, STOI, F0-RMSE, V/UV F1

### Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Audio (16kHz) â†’ WavLM-Base+ (frozen) â†’ Layer Selection        â”‚
â”‚       â†“                                                          â”‚
â”‚  Learned Fusion (Î±â‚hâ‚ + ... + Î±â‚™hâ‚™) â†’ Adapter (768â†’256)       â”‚
â”‚       â†“                                                          â”‚
â”‚  HiFi-GAN Generator (Ã—320 upsampling) â†’ Reconstructed Audio    â”‚
â”‚       â†“                                                          â”‚
â”‚  [Optional] MPD/MSD Discriminators + Feature Matching          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Results Summary

| Configuration | MCDâ†“ | Mel-L1â†“ | PESQâ†‘ | STOIâ†‘ | V/UV F1â†‘ | F0 RMSEâ†“ | F0 Corrâ†‘ |
|--------------|------|---------|-------|-------|----------|----------|----------|
| **No GAN** | 9.72 | 1.55 | 1.11 | 0.74 | 0.878 | 10.1 | 0.83 |
| **+MPD/MSD+FM** | **8.43** | **1.17** | **1.28** | **0.86** | **0.932** | **7.7** | **0.96** |
| **Gain** | -13.3% | -24.5% | +15.3% | +16.2% | +6.1% | -23.8% | +15.7% |

> **Key Findings**:
> - Adversarial supervision (GAN) provides **consistent gains** across all metrics
> - Layers 7-12 capture most phonetic-prosodic information
> - Learned layer fusion outperforms fixed single-layer extraction

---

## ğŸš€ Quick Start

### 1. Installation
```bash
# Clone repository
git clone https://github.com/NassimaOULDOUALI/wavlm-vocoder-french.git
cd wavlm-vocoder-french

# Install dependencies
pip install -r requirements.txt

# Install package in editable mode
pip install -e .
```

See [docs/INSTALL.md](docs/INSTALL.md) for detailed setup instructions.

### 2. Data Preparation
```bash
# Download datasets
bash scripts/download_data.sh

# Preprocess audio (normalize, filter, detect silence)
python scripts/preprocess_data.py \
    --input_dir data/raw \
    --output_dir data/processed \
    --sample_rate 16000
```

### 3. Training
```bash
# Train without GAN (baseline)
bash scripts/train_model.sh --config configs/no_gan_config.yaml

# Train with GAN supervision
bash scripts/train_model.sh --config configs/with_gan_config.yaml

# Layer ablation experiments
bash scripts/train_model.sh --config configs/layer_ablation_config.yaml --num_layers 9
```

See [docs/TRAINING.md](docs/TRAINING.md) for advanced training options.

### 4. Evaluation
```bash
# Evaluate on test set
python scripts/evaluate_model.sh \
    --checkpoint outputs/checkpoints/best_model.pt \
    --test_dir data/processed/test \
    --output_dir outputs/evaluation
```

See [docs/EVALUATION.md](docs/EVALUATION.md) for metric details.

### 5. Inference
```bash
# Reconstruct audio from WavLM representations
python scripts/inference.py \
    --checkpoint outputs/checkpoints/best_model.pt \
    --input_audio examples/sample.wav \
    --output_audio outputs/samples/reconstructed.wav \
    --num_layers 9
```

---

## ğŸ“ Repository Structure
```
wavlm-vocoder-french/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ adapter.py              # WavLM adapter (768â†’256)
â”‚   â”‚   â”œâ”€â”€ generator.py            # HiFi-GAN generator
â”‚   â”‚   â”œâ”€â”€ discriminator.py        # MPD/MSD discriminators
â”‚   â”‚   â”œâ”€â”€ wavlm_vocoder.py        # Main vocoder class
â”‚   â”‚   â””â”€â”€ layer_fusion.py         # Learned layer weighting
â”‚   â”œâ”€â”€ dataset/
â”‚   â”‚   â”œâ”€â”€ audio_dataset.py        # PyTorch dataset
â”‚   â”‚   â””â”€â”€ preprocessing.py        # Audio cleaning pipeline
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ train.py                # Training script
â”‚   â”‚   â”œâ”€â”€ losses.py               # L1, Mel, STFT, Adv, FM losses
â”‚   â”‚   â””â”€â”€ trainer.py              # DDP/AMP trainer
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ metrics.py              # MCD, PESQ, STOI, F0, V/UV
â”‚   â”‚   â””â”€â”€ evaluate.py             # Evaluation pipeline
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ audio_processing.py     # Chunk inference, windowing
â”‚       â””â”€â”€ logger.py               # TensorBoard logging
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ base_config.yaml            # Shared hyperparameters
â”‚   â”œâ”€â”€ no_gan_config.yaml          # Baseline (spectral losses only)
â”‚   â”œâ”€â”€ with_gan_config.yaml        # Full model (MPD/MSD+FM)
â”‚   â””â”€â”€ layer_ablation_config.yaml  # Layer sweep experiments
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_data.sh            # Download SIWIS, M-AILABS, CommonVoice
â”‚   â”œâ”€â”€ preprocess_data.py          # Audio cleaning & filtering
â”‚   â”œâ”€â”€ train_model.sh              # Multi-GPU training launcher
â”‚   â”œâ”€â”€ evaluate_model.sh           # Compute all metrics
â”‚   â””â”€â”€ inference.py                # Reconstruct from checkpoint
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 1_data_exploration.ipynb    # Corpus statistics
â”‚   â”œâ”€â”€ 2_model_architecture.ipynb  # Visualize layers, weights
â”‚   â””â”€â”€ 3_results_analysis.ipynb    # Plot metrics, ablations
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_models.py              # Unit tests for architectures
â”‚   â”œâ”€â”€ test_dataset.py             # Dataset loading tests
â”‚   â””â”€â”€ test_losses.py              # Loss function tests
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ INSTALL.md                  # Installation guide
â”‚   â”œâ”€â”€ TRAINING.md                 # Training instructions
â”‚   â”œâ”€â”€ EVALUATION.md               # Metrics & evaluation
â”‚   â”œâ”€â”€ REPRODUCTION.md             # Reproduce paper results
â”‚   â””â”€â”€ paper/
â”‚       â””â”€â”€ JEP_2026_WavLM_Vocodeur.pdf  # Accepted paper
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ setup.py                        # Package setup
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ LICENSE                         # MIT License
â””â”€â”€ CITATION.bib                    # BibTeX citation
```

---

## ğŸ“– Documentation

- **[Installation Guide](docs/INSTALL.md)**: Detailed setup (CUDA, dependencies, data)
- **[Training Guide](docs/TRAINING.md)**: Hyperparameters, DDP/AMP, early stopping
- **[Evaluation Guide](docs/EVALUATION.md)**: Metric calculation, test protocols
- **[Reproduction Guide](docs/REPRODUCTION.md)**: Exact commands to reproduce paper results

---

## ğŸ”¬ Key Experiments

### Experiment 1: GAN vs. No-GAN
```bash
# Baseline (spectral losses only)
python src/training/train.py --config configs/no_gan_config.yaml

# With adversarial supervision
python src/training/train.py --config configs/with_gan_config.yaml
```

**Result**: GAN provides 13-24% improvement in spectral fidelity and 16% in perceptual quality.

### Experiment 2: Layer Ablation
```bash
# Test N=1,2,...,12 last layers
for N in {1..12}; do
    python src/training/train.py \
        --config configs/layer_ablation_config.yaml \
        --num_layers $N
done
```

**Result**: Layers 9-12 are optimal (sufficient information without redundancy).

### Experiment 3: Learned Fusion vs. Averaging
```yaml
# In config file:
layer_fusion:
  mode: "learned"  # or "average"
  num_layers: 9
```

**Result**: Learned fusion (weighted Î±) outperforms simple averaging.

---

## ğŸ“¦ Pretrained Checkpoints

| Model | Layers | GAN | MCD | PESQ | Download |
|-------|--------|-----|-----|------|----------|
| Baseline | 12 | âŒ | 9.72 | 1.11 | [Link](#) |
| Best (N=9) | 9 | âœ… | **8.43** | **1.28** | [Link](#) |
| Lightweight (N=6) | 6 | âœ… | 8.89 | 1.21 | [Link](#) |

---

## ğŸ“ Citation

If you use this work, please cite our paper:
```bibtex
@inproceedings{wavlm_vocoder_french_2026,
  title={Vocodage WavLM vers audio en franÃ§ais : Ablation des couches et supervision adversariale comme fondation pour la conversion de voix continue},
  author={Nassima OULD OUALI, Awais Hussein Sani, Reda Dehak, Eric Moulines},
  booktitle={JournÃ©es d'Ã‰tudes sur la Parole (JEP)},
  year={2026}
}
```

---

## ğŸ“œ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) file.

---

## ğŸ™ Acknowledgments

- **WavLM**: Microsoft Research ([Chen et al., 2022](https://arxiv.org/abs/2110.13900))
- **HiFi-GAN**: [Kong et al., 2020](https://arxiv.org/abs/2010.05646)
- **Datasets**: SIWIS, M-AILABS, Common Voice
- **Metrics**: WORLD vocoder, CREPE pitch tracker

---

## ğŸ› ï¸ Contributing

Contributions are welcome! Please see our [Contributing Guidelines](CONTRIBUTING.md).

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“§ Contact

For questions, please open an issue or contact: **nassima.ould-ouali@ip-paris.fr**

---

## ğŸ—ºï¸ Roadmap

- [x] Stage 1: Reconstruction vocoder (this work)
- [ ] Stage 2: Voice conversion in WavLM latent space
- [ ] Stage 3: Diffusion/Flow-based manipulation
- [ ] Real-time inference optimization

---

**Star â­ this repo if you find it useful!**
